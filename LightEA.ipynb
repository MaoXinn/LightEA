{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the GPU, \"-1\" represents using the CPU\n",
    "\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# import all the requirements\n",
    "import faiss \n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type=\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the dataset and set the random seed\n",
    "# the first run may be slow because the graph needs to be preprocessed into binary cache\n",
    "\n",
    "np.random.seed(12306)\n",
    "dataset = [\"DBP_ZH_EN/\",\"DBP_JA_EN/\",\"DBP_FR_EN/\",\"SRPRS_FR_EN/\",\"SRPRS_DE_EN/\",\"DBP_WD/\",\"DBP_YG/\"][2]\n",
    "path = \"./EA_datasets/\"+ dataset\n",
    "\n",
    "# set hyper-parameters, load graphs and pre-aligned entity pairs\n",
    "# if your GPU is out of memory, try to reduce the ent_dim\n",
    "\n",
    "ent_dim, depth, top_k = 1024, 2, 500\n",
    "if \"EN\" in dataset:\n",
    "    rel_dim, mini_dim = ent_dim//2, 16\n",
    "else:\n",
    "    rel_dim, mini_dim = ent_dim//3, 16\n",
    "    \n",
    "node_size, rel_size, ent_tuple, triples_idx, ent_ent, ent_ent_val, rel_ent, ent_rel = load_graph(path)\n",
    "\n",
    "train_pair,test_pair = load_aligned_pair(path,ratio=0.30)\n",
    "candidates_x,candidates_y = set([x for x,y in test_pair]), set([y for x,y in test_pair]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 µs, sys: 0 ns, total: 19 µs\n",
      "Wall time: 37.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# main functions of LightEA\n",
    "\n",
    "def random_projection(x,out_dim):\n",
    "    random_vec = K.l2_normalize(tf.random.normal((x.shape[-1],out_dim)),axis=-1)\n",
    "    return K.dot(x,random_vec)\n",
    "\n",
    "def batch_sparse_matmul(sparse_tensor,dense_tensor,batch_size = 128,save_mem = False):\n",
    "    results = []\n",
    "    for i in range(dense_tensor.shape[-1]//batch_size + 1):\n",
    "        temp_result = tf.sparse.sparse_dense_matmul(sparse_tensor,dense_tensor[:, i*batch_size:(i+1)*batch_size])\n",
    "        if save_mem:\n",
    "            temp_result = temp_result.numpy()\n",
    "        results.append(temp_result)\n",
    "    if save_mem:\n",
    "        return np.concatenate(results,-1)\n",
    "    else:\n",
    "        return K.concatenate(results,-1)\n",
    "\n",
    "def get_features(train_pair,extra_feature = None):\n",
    "    \n",
    "    if extra_feature is not None:\n",
    "        ent_feature = extra_feature\n",
    "    else:\n",
    "        random_vec = K.l2_normalize(tf.random.normal((len(train_pair),ent_dim)),axis=-1)\n",
    "        ent_feature = tf.tensor_scatter_nd_update(tf.zeros((node_size,ent_dim)),train_pair.reshape((-1,1)),tf.repeat(random_vec,2,axis=0))\n",
    "    rel_feature = tf.zeros((rel_size,ent_feature.shape[-1]))\n",
    "    \n",
    "    ent_ent_graph = tf.SparseTensor(indices=ent_ent,values=ent_ent_val,dense_shape=(node_size,node_size))\n",
    "    rel_ent_graph = tf.SparseTensor(indices=rel_ent,values=K.ones(rel_ent.shape[0]),dense_shape=(rel_size,node_size))\n",
    "    ent_rel_graph = tf.SparseTensor(indices=ent_rel,values=K.ones(ent_rel.shape[0]),dense_shape=(node_size,rel_size))\n",
    "    \n",
    "    ent_list,rel_list = [ent_feature],[rel_feature]\n",
    "    for i in range(2):\n",
    "        new_rel_feature = batch_sparse_matmul(rel_ent_graph,ent_feature)\n",
    "        new_rel_feature = tf.nn.l2_normalize(new_rel_feature,axis=-1)\n",
    "        \n",
    "        new_ent_feature = batch_sparse_matmul(ent_ent_graph,ent_feature)\n",
    "        new_ent_feature += batch_sparse_matmul(ent_rel_graph,rel_feature)\n",
    "        new_ent_feature = tf.nn.l2_normalize(new_ent_feature,axis=-1)\n",
    "        \n",
    "        ent_feature = new_ent_feature; rel_feature = new_rel_feature\n",
    "        ent_list.append(ent_feature); rel_list.append(rel_feature)\n",
    "    \n",
    "    ent_feature = K.l2_normalize(K.concatenate(ent_list,1),-1)\n",
    "    rel_feature = K.l2_normalize(K.concatenate(rel_list,1),-1)\n",
    "    rel_feature = random_projection(rel_feature,rel_dim)\n",
    "    \n",
    "    \n",
    "    batch_size = ent_feature.shape[-1]//mini_dim\n",
    "    sparse_graph = tf.SparseTensor(indices=triples_idx,values=K.ones(triples_idx.shape[0]),dense_shape=(np.max(triples_idx)+1,rel_size))\n",
    "    adj_value = batch_sparse_matmul(sparse_graph,rel_feature)\n",
    "    \n",
    "    features_list = []\n",
    "    for batch in range(rel_dim//batch_size + 1):\n",
    "        temp_list = []\n",
    "        for head in range(batch_size):\n",
    "            if batch*batch_size+head>=rel_dim:\n",
    "                break\n",
    "            sparse_graph = tf.SparseTensor(indices=ent_tuple,values=adj_value[:,batch*batch_size+head],dense_shape=(node_size,node_size))\n",
    "            feature = batch_sparse_matmul(sparse_graph,random_projection(ent_feature,mini_dim))\n",
    "            temp_list.append(feature)\n",
    "        if len(temp_list):\n",
    "            features_list.append(K.concatenate(temp_list,-1).numpy())\n",
    "    features = np.concatenate(features_list,axis=-1)\n",
    "    \n",
    "    faiss.normalize_L2(features)\n",
    "    if extra_feature is not None:\n",
    "        features = np.concatenate([ent_feature,features],axis=-1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the literal features of entities, only work on DBP15K & SRPRS\n",
    "# for the first run, you need to download the pre-train word embeddings from \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "# unzip this file and put \"glove.6B.300d.txt\" into the root of LightEA\n",
    "\n",
    "using_name_features = False\n",
    "if using_name_features and \"EN\" in dataset: \n",
    "    name_features = load_name_features(dataset,\"./glove.6B.300d.txt\",mode = \"hybrid-level\")\n",
    "    l_features = get_features(train_pair,extra_feature = name_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1 start:\n",
      "new generated pairs = 7092\n",
      "rest pairs = 3408\n",
      "Hits@1: 0.754 Hits@10: 0.903 MRR: 0.808\n",
      "\n",
      "Round 2 start:\n",
      "new generated pairs = 1056\n",
      "rest pairs = 2352\n",
      "Hits@1: 0.806 Hits@10: 0.915 MRR: 0.847\n",
      "\n",
      "Round 3 start:\n",
      "Hits@1: 0.806 Hits@10: 0.912 MRR: 0.846\n",
      "\n",
      "CPU times: user 1min 36s, sys: 6.34 s, total: 1min 42s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Obtain the structural features and iteratively generate Semi-supervised data\n",
    "# \"epoch = 1\" represents removing the iterative strategy\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(\"Round %d start:\"%(epoch+1))\n",
    "    s_features = get_features(train_pair)  \n",
    "    if using_name_features and \"EN\" in dataset:\n",
    "        features = np.concatenate([s_features,l_features],-1)\n",
    "    else:\n",
    "        features = s_features\n",
    "    if epoch < epochs-1:\n",
    "        left,right = list(candidates_x),list(candidates_y)\n",
    "        index,sims = sparse_sinkhorn_sims(left,right,features,top_k)\n",
    "        ranks = tf.argsort(-sims,-1).numpy()\n",
    "        sims = sims.numpy(); index = index.numpy()\n",
    "\n",
    "        temp_pair = []\n",
    "        x_list,y_list= list(candidates_x),list(candidates_y)\n",
    "        for i in range(ranks.shape[0]):\n",
    "            if sims[i,ranks[i,0]] > 0.5:\n",
    "                x = x_list[i]\n",
    "                y = y_list[index[i,ranks[i,0]]]\n",
    "                temp_pair.append((x,y))\n",
    "\n",
    "        for x,y in temp_pair:\n",
    "            if x in candidates_x:\n",
    "                candidates_x.remove(x);\n",
    "            if y in candidates_y:\n",
    "                candidates_y.remove(y);\n",
    "        \n",
    "        print(\"new generated pairs = %d\"%(len(temp_pair)))\n",
    "        print(\"rest pairs = %d\"%(len(candidates_x)))\n",
    "        \n",
    "        if not len(temp_pair):\n",
    "            break\n",
    "        train_pair = np.concatenate([train_pair,np.array(temp_pair)])\n",
    "        \n",
    "    right_list, wrong_list = test(test_pair,features,top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.6",
   "language": "python",
   "name": "tf2.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
